{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Автор: Анна Смелова**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение домашнего задания к уроку 8 \"Снижение размерности данных\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "#### Обучить любую модель классификации на датасете IRIS до применения PCA (2 компоненты) и после него. Сравнить качество классификации по отложенной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для подсчета точности как доли правильных ответов\n",
    "def get_accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# масштабирование\n",
    "def standard_scale(x):\n",
    "    res = (x - x.mean(axis=0)) / x.std(axis=0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ковариация\n",
    "def covariance(x, y):\n",
    "    return np.sum(x * y) / (len(x) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс узла\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, index, t, true_branch, false_branch):\n",
    "        self.index = index  # индекс признака, по которому ведется сравнение с порогом в этом узле\n",
    "        self.t = t  # значение порога\n",
    "        self.true_branch = true_branch  # поддерево, удовлетворяющее условию в узле\n",
    "        self.false_branch = false_branch  # поддерево, не удовлетворяющее условию в узле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс терминального узла (листа)\n",
    "class Leaf:\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.prediction = self.predict()\n",
    "        \n",
    "    def predict(self):\n",
    "        # подсчет количества объектов разных классов\n",
    "        classes = {}  # сформируем словарь \"класс: количество объектов\"\n",
    "        for label in self.labels:\n",
    "            if label not in classes:\n",
    "                classes[label] = 0\n",
    "            classes[label] += 1\n",
    "            \n",
    "        # найдем класс, количество объектов которого будет максимальным в этом листе и вернем его    \n",
    "        prediction = max(classes, key=classes.get)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс дерева\n",
    "class Tree:\n",
    "    \n",
    "    def __init__(self, max_depth=100): # максимальная глубина дерева\n",
    "            \n",
    "        self.max_depth = max_depth\n",
    "        self.nodes = []\n",
    "        self.leaves = []\n",
    "        self.depth = 0\n",
    "        self.tree = None\n",
    "    \n",
    "    # Расчет критерия Джини\n",
    "    def gini(self, labels):\n",
    "        #  подсчет количества объектов разных классов\n",
    "        classes = {}\n",
    "        for label in labels:\n",
    "            if label not in classes:\n",
    "                classes[label] = 0\n",
    "            classes[label] += 1\n",
    "\n",
    "        #  расчет критерия\n",
    "        impurity = 1\n",
    "        for label in classes:\n",
    "            p = classes[label] / len(labels)\n",
    "            impurity -= p ** 2\n",
    "\n",
    "        return impurity\n",
    "    \n",
    "    # Расчет прироста\n",
    "    def gain(self, left_labels, right_labels, root_gini):\n",
    "        # доля выборки, ушедшая в левое поддерево\n",
    "        p = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])\n",
    "\n",
    "        return root_gini - p * self.gini(left_labels) - (1 - p) * self.gini(right_labels)\n",
    "    \n",
    "    # Разбиение датасета в узле\n",
    "    def split(self, data, labels, column_index, t):\n",
    "        left = np.where(data[:, column_index] <= t)\n",
    "        right = np.where(data[:, column_index] > t)\n",
    "\n",
    "        true_data = data[left]\n",
    "        false_data = data[right]\n",
    "\n",
    "        true_labels = labels[left]\n",
    "        false_labels = labels[right]\n",
    "\n",
    "        return true_data, false_data, true_labels, false_labels\n",
    "    \n",
    "    # Реализуем генерацию подмножества признаков для нахождения разбиения в узле\n",
    "    def get_subsample(self, len_sample):\n",
    "        # будем сохранять не сами признаки, а их индексы\n",
    "        sample_indexes = list(range(len_sample))\n",
    "\n",
    "        len_subsample = int(np.sqrt(len_sample))\n",
    "\n",
    "        subsample = np.random.choice(sample_indexes, size=len_subsample, replace=False)\n",
    "\n",
    "        return subsample\n",
    "    \n",
    "    # Нахождение наилучшего разбиения\n",
    "    def find_best_split(self, data, labels):\n",
    "        #  обозначим минимальное количество объектов в узле\n",
    "        min_leaf_samples = 5\n",
    "\n",
    "        root_gini = self.gini(labels)\n",
    "\n",
    "        best_gain = 0\n",
    "        best_t = None\n",
    "        best_index = None\n",
    "\n",
    "        n_features = data.shape[1]\n",
    "\n",
    "        feature_subsample_indices = self.get_subsample(n_features) # выбираем случайные признаки\n",
    "\n",
    "        for index in feature_subsample_indices:\n",
    "            # будем проверять только уникальные значения признака, исключая повторения\n",
    "            t_values = np.unique([obj[index] for obj in data])\n",
    "\n",
    "            for t in t_values:\n",
    "                true_data, false_data, true_labels, false_labels = self.split(data, labels, index, t)\n",
    "                #  пропускаем разбиения, в которых в узле остается менее 5 объектов\n",
    "                if len(true_data) < min_leaf_samples or len(false_data) < min_leaf_samples:\n",
    "                    continue\n",
    "\n",
    "                current_gain = self.gain(true_labels, false_labels, root_gini)\n",
    "\n",
    "                #  выбираем порог, на котором получается максимальный прирост качества\n",
    "                if current_gain > best_gain:\n",
    "                    best_gain, best_t, best_index = current_gain, t, index\n",
    "\n",
    "        return best_gain, best_t, best_index\n",
    "    \n",
    "    # Построение дерева с помощью рекурсивной функции\n",
    "    def build_tree(self, data, labels):\n",
    "        gain, t, index = self.find_best_split(data, labels)\n",
    "        \n",
    "        #  Базовый случай 1 - прекращаем рекурсию, когда нет прироста в качества\n",
    "        if gain == 0:\n",
    "            self.leaves.append(Leaf(data, labels))\n",
    "            return Leaf(data, labels)\n",
    " \n",
    "        #  Базовый случай 2 - прекращаем рекурсию, когда достигли максимальной глубины дерева\n",
    "        if self.depth >= self.max_depth:\n",
    "            self.leaves.append(Leaf(data, labels))\n",
    "            return Leaf(data, labels)\n",
    "\n",
    "        self.depth += 1\n",
    "        \n",
    "        true_data, false_data, true_labels, false_labels = self.split(data, labels, index, t)\n",
    "\n",
    "        # Рекурсивно строим два поддерева\n",
    "        true_branch = self.build_tree(true_data, true_labels)\n",
    "        false_branch = self.build_tree(false_data, false_labels)\n",
    "\n",
    "        # Возвращаем класс узла со всеми поддеревьями, то есть целого дерева\n",
    "        self.nodes.append(Node(index, t, true_branch, false_branch))\n",
    "        return Node(index, t, true_branch, false_branch)\n",
    "    \n",
    "    def classify_object(self, obj, node):\n",
    "        #  Останавливаем рекурсию, если достигли листа\n",
    "        if isinstance(node, Leaf):\n",
    "            answer = node.prediction\n",
    "            return answer\n",
    "\n",
    "        if obj[node.index] <= node.t:\n",
    "            return self.classify_object(obj, node.true_branch)\n",
    "        else:\n",
    "            return self.classify_object(obj, node.false_branch)\n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        self.tree = self.build_tree(data, labels)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, data):\n",
    "        classes = []\n",
    "        for obj in data:\n",
    "            prediction = self.classify_object(obj, self.tree)\n",
    "            classes.append(prediction)\n",
    "        return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс случайного леса\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees, # количествоо деревьев \n",
    "                 max_depth=100, # максимальная глубина деревьев\n",
    "                 max_leaves=200, # максимальное количество листьев в деревьях\n",
    "                 min_leaf_objects=1):  # минимальное количество объектов в листе\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.max_leaves = max_leaves\n",
    "        self.min_leaf_objects = min_leaf_objects\n",
    "        self.forest = [] \n",
    "        self.train_accuracy = []\n",
    "        self.test_accuracy = []\n",
    "        self.train_roc = []\n",
    "        self.test_roc = []\n",
    "    \n",
    "    # Реализуем генерацию 𝑁 бутстрап-выборок\n",
    "    def get_bootstrap(self, data, labels, N):\n",
    "        n_samples = data.shape[0] # размер совпадает с исходной выборкой\n",
    "        bootstrap = []\n",
    "\n",
    "        for i in range(N):\n",
    "            sample_index = np.random.randint(0, n_samples, size=n_samples)\n",
    "            b_data = data[sample_index]\n",
    "            b_labels = labels[sample_index]\n",
    "\n",
    "            bootstrap.append((b_data, b_labels))\n",
    "\n",
    "        return bootstrap\n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        forest = []\n",
    "        bootstrap = self.get_bootstrap(data, labels, self.n_trees)\n",
    "\n",
    "        for b_data, b_labels in bootstrap:\n",
    "            new_tree = Tree(self.max_depth)\n",
    "            new_tree.fit(b_data, b_labels)\n",
    "            self.forest.append(new_tree)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    # предсказание голосованием деревьев\n",
    "    def tree_vote(self, data):\n",
    "        # добавим предсказания всех деревьев в список\n",
    "        predictions = []\n",
    "        for tree in self.forest:\n",
    "            \n",
    "            tree_vote = tree.predict(data)\n",
    "            predictions.append(tree_vote)\n",
    "\n",
    "        # сформируем список с предсказаниями для каждого объекта\n",
    "        predictions_per_object = list(zip(*predictions))\n",
    "\n",
    "        # выберем в качестве итогового предсказания для каждого объекта то,\n",
    "        # за которое проголосовало большинство деревьев\n",
    "        voted_predictions = []\n",
    "        for obj in predictions_per_object:\n",
    "            voted_predictions.append(max(set(obj), key=obj.count))\n",
    "\n",
    "        return voted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим датасет iris\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# основная информация по датасету\n",
    "print(data.shape, target.shape)\n",
    "print(target_names)\n",
    "display(data[:5,:], target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90068117,  1.01900435, -1.34022653, -1.3154443 ],\n",
       "       [-1.14301691, -0.13197948, -1.34022653, -1.3154443 ],\n",
       "       [-1.38535265,  0.32841405, -1.39706395, -1.3154443 ],\n",
       "       [-1.50652052,  0.09821729, -1.2833891 , -1.3154443 ],\n",
       "       [-1.02184904,  1.24920112, -1.34022653, -1.3154443 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# масштабируем\n",
    "data = standard_scale(data)\n",
    "display(data[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (45, 4) (105,) (45,)\n"
     ]
    }
   ],
   "source": [
    "# перемешивание датасета\n",
    "np.random.seed(0)\n",
    "shuffle_index = np.random.permutation(data.shape[0])\n",
    "X, y = data[shuffle_index], target[shuffle_index]\n",
    "\n",
    "# разделим датасет на обучающую и отложенную выборки\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 98.09523809523809\n"
     ]
    }
   ],
   "source": [
    "# Обучим RandomForest до применения PCA\n",
    "clf_before = RandomForest(n_trees=20)\n",
    "clf_before.fit(X_train, y_train)\n",
    "train_labels = clf_before.tree_vote(X_train)\n",
    "print(f'Train accuracy {get_accuracy_metric(y_train, train_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные значения и собственные векторы в порядке убывания:\n",
      "(437.77467247979905, array([ 0.52106591, -0.26934744,  0.5804131 ,  0.56485654]))\n",
      "(137.1045707202107, array([-0.37741762, -0.92329566, -0.02449161, -0.06694199]))\n",
      "(22.01353133569727, array([-0.71956635,  0.24438178,  0.14212637,  0.63427274]))\n",
      "(3.107225464292895, array([ 0.26128628, -0.12350962, -0.80144925,  0.52359713]))\n"
     ]
    }
   ],
   "source": [
    "# найдем собственные векторы и собственные значения\n",
    "covariance_matrix = X.T @ X\n",
    "\n",
    "eig_values, eig_vectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# сформируем список кортежей (собственное значение, собственный вектор)\n",
    "eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
    "\n",
    "# и отсортируем список по убыванию собственных значений\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('Собственные значения и собственные векторы в порядке убывания:')\n",
    "for i in eig_pairs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля дисперсии, описываемая каждой из компонент \n",
      "[72.96244541329983, 22.85076178670178, 3.6689218892828785, 0.5178709107154825]\n",
      "Кумулятивная доля дисперсии по компонентам \n",
      "[ 72.96244541  95.8132072   99.48212909 100.        ]\n"
     ]
    }
   ],
   "source": [
    "# оценим долю дисперсии, которая описывается найденными компонентами.\n",
    "eig_sum = sum(eig_values)\n",
    "var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Доля дисперсии, описываемая каждой из компонент \\n{var_exp}')\n",
    "\n",
    "# а теперь оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
    "print(f'Кумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, первая главная компонента описывает почти 73% информации, а первые две в сумме - 95.8%. В то же время последняя компонента описывает всего 0.5% и может быть отброжена без страха значительных потерь в качестве нашего анализа. Мы отбросим последние две компоненты, оставив первые две."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица весов W:\n",
      " [[ 0.52106591 -0.37741762]\n",
      " [-0.26934744 -0.92329566]\n",
      " [ 0.5804131  -0.02449161]\n",
      " [ 0.56485654 -0.06694199]]\n"
     ]
    }
   ],
   "source": [
    "# Сформируем вектор весов из собственных векторов, соответствующих первым двум главным компонентам\n",
    "W = np.hstack([eig_pairs[i][1].reshape(4,1) for i in range(2)])\n",
    "\n",
    "print(f'Матрица весов W:\\n', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4676452 ,  0.44227159],\n",
       "       [ 0.56210831,  1.76472438],\n",
       "       [-2.44617739, -2.15072788],\n",
       "       [ 2.30243944, -0.42006558],\n",
       "       [-2.23284716, -0.22314807]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сформируем новую матрицу \"объекты-признаки\"\n",
    "X_reduced = X.dot(W)\n",
    "X_reduced[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 2) (45, 2) (105,) (45,)\n"
     ]
    }
   ],
   "source": [
    "# разделим датасет на обучающую и отложенную выборки\n",
    "X_train_reduced, X_valid_reduced, y_train_reduced, y_valid_reduced = train_test_split(X_reduced, y, test_size=0.3, random_state=42)\n",
    "print(X_train_reduced.shape, X_valid_reduced.shape, y_train_reduced.shape, y_valid_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 95.23809523809523\n"
     ]
    }
   ],
   "source": [
    "# Обучим RandomForest после применения PCA\n",
    "clf_after = RandomForest(n_trees=20)\n",
    "clf_after.fit(X_train_reduced, y_train_reduced)\n",
    "train_labels_reduced = clf_after.tree_vote(X_train_reduced)\n",
    "print(f'Train accuracy {get_accuracy_metric(y_train_reduced, train_labels_reduced)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy before PCA: 91.11111111111111, Train: 98.09523809523809\n",
      "Valid accuracy after PCA: 95.55555555555556, Train: 95.23809523809523\n"
     ]
    }
   ],
   "source": [
    "# точность на отложенной выборке\n",
    "test_labels = clf_before.tree_vote(X_valid)\n",
    "test_labels_reduced = clf_after.tree_vote(X_valid_reduced)\n",
    "print(f'Valid accuracy before PCA: {get_accuracy_metric(y_valid, test_labels)}, Train: {get_accuracy_metric(y_train, train_labels)}')\n",
    "print(f'Valid accuracy after PCA: {get_accuracy_metric(y_valid_reduced, test_labels_reduced)}, Train: {get_accuracy_metric(y_train_reduced, train_labels_reduced)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: после применения PCA качество классификации выросло. Также уменьшилось переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
